simulate_stock <- function(
  tsa_fit, seed, n_sim = 1000, type = c("fixed_U_Y", "F_at_age"), n_min_factor = 0.2) {
  
  #### simulates N and F based on a TSA fit ####
  
  # type options:
  # fixed_U_Y uses the estimates of U and Y from the model fit and adds transitory noise
  # F_at_age uses the covariance matrix of the estimates of log F 
  # - leads to estimates of F that are too variable because it ignores temporal correlation 
  
  # n_min_factor ensures no simulated values of N are negative; e.g. recruitment can't be 
  # less than the estimate of recruitment * n_min_factor
  
  type <- match.arg(type)
  
  set.seed(seed)
  
  
  #### get estimates of(log) F, U, V Y from model fit ####
  
  # need to run special tsa dll :)
  # first get tsa_setup object and replace param with final parameter estimates
  # this avoids having to minimise the likelihood (again)
  # creates two files "TSA F estimates.txt" and "TSA U V Y estimates.txt" in working directory
  
  tsa_setup <- tsa_fit$data
  tsa_setup$param <- tsa.update.param(tsa_setup, tsa_fit, "estimates")$param
  
  wk <- tsa.fit(tsa_setup, dll = "TSA get F U V Y.dll", filterType = "backward")
  rm(wk)
  
  # read in F or U and Y from txt files
  # object is a list:
  #   estimate are estimates by year and age (or U at age, V, Y)
  #   vcov are the between-age covariance matrices by year
  
  F_data <- read_F(tsa_fit, type)
  
  
  #### simulate F ####
  
  F_sim <- switch(
    type, 
    fixed_U_Y = simulate_fixed_U_Y(tsa_fit, F_data, n_sim), 
    F_at_age = simulate_F_at_age(tsa_fit, F_data, n_sim)
  )
  
  
  #### simulate N ####
  
  # output is a list of values of N and F
  
  lapply(F_sim, simulate_N, tsa_fit = tsa_fit, n_min_factor = n_min_factor)
}


get_tsa <- function(tsa_fit, type) {

  # gets useful info from a tsa model fit 
  
  with(tsa_fit, switch(  
    type, 
    n_year = diff(data$ylim) + 1,
    n_age = diff(data$alim) + 1,
    n_u = max(data$f.est) - min(data$f.est),
    n_uvy = max(data$f.est) - min(data$f.est) + 2,
    sim_obj = {
      # empty structure to hold simulated n and f
      array(dim  = dim(n$estimate), dimnames = dimnames(n$estimate))
    },
    y_labels = dimnames(n$estimate)[[1]],
    a_labels = dimnames(n$estimate)[[2]],
    u_labels = paste0("U", seq(min(data$f.est), max(data$f.est) - 1)),
    uvy_labels = c(paste0("U", seq(min(data$f.est), max(data$f.est) - 1)), "V", "Y")
  ))
}  


read_F <- function(tsa_fit, type) {
  
  # read in annual estimates of F (or of U, V and Y) and their variance covariance matrix
  # data stored as row of estimates for year 1, variance for year 1, estimates for year 2 etc.
  
  # note - these are the estimates from the state space vector, not those generated by 
  # the sigma points adn e.g. reported in model$f$estimate
  
  infile <- switch(type, fixed_U_Y = "TSA U V Y estimates.txt", F_at_age = "TSA F estimates.txt")
  data <- read.table(infile)
  

  # set up suitable structure to hold estimates and variances

  n_year <- get_tsa(tsa_fit, "n_year")
  y_labels <- get_tsa(tsa_fit, "y_labels")
  
  n_age <- get_tsa(tsa_fit, switch(type, fixed_U_Y = "n_uvy", F_at_age = "n_age"))
  a_labels <- get_tsa(tsa_fit, switch(type, fixed_U_Y = "uvy_labels", F_at_age = "a_labels"))
  
  estimate <- array(dim = c(n_year, n_age), dimnames = list(y_labels, a_labels))
  vcov <- array(dim = c(n_year, n_age, n_age), dimnames = list(y_labels, a_labels, a_labels))
  

  # pick out relevant rows to give estimates
  
  id <- 1:n_year * (n_age + 1) - n_age
  estimate[] <- as.matrix(data[id, ])
  
  # strip out rows to just leave variances
  
  data <- data[- id, ]
  
  # allocate variances
  
  ymin <- tsa_fit$data$ylim[1]
  for (iy in 1:n_year) {
    id <- as.character(ymin + iy - 1)
    vcov[id, , ] <- as.matrix(data[(1:n_age) + (iy - 1) * n_age, ])
  }
  
  list(estimate = estimate, vcov = vcov)
}


simulate_F_at_age <- function(tsa_fit, F_data, n_sim) {
  
  # simulate log F based on estimates of log F at age and their within-year variance
  # then backtransform
  
  require(MASS)
  
  y_labels <- get_tsa(tsa_fit, "y_labels")
  out <- get_tsa(tsa_fit, "sim_obj")
  
  replicate(n_sim, {
    for (iy in y_labels) 
      out[iy, ] <- mvrnorm(1, F_data$estimate[iy, ], F_data$vcov[iy, , ])
    exp(out)
  }, simplify = FALSE)
}


simulate_fixed_U_Y <- function(tsa_fit, F_data, n_sim) {
  
  # simulate F based on estimates of U and Y and transistory variation in F
  # then backtransform

  out <- get_tsa(tsa_fit, "sim_obj")

  n_u <- get_tsa(tsa_fit, "n_u")
  n_age <- get_tsa(tsa_fit, "n_age")
  n_year <- get_tsa(tsa_fit, "n_year")
  
  
  # persistent estimates of log F (i.e. U + Y)
  
  Y <- U <- out
    
  Y[] <- F_data$estimate[, "Y"]
    
  U[, 1:n_u] <- F_data$estimate[, 1:n_u]
  
  # U for ages in min(f.est) to max(f.est) must sum to zero 
  # U for higher ages equals U for max(f.est)
  
  U[, (n_u+1):n_age] <- - rowSums(U[, 1:n_u])
    
  F_pers <- U + Y
  

  # transitory noise 

  sd_v <- tsa_fit$summary["sd V", "estimate"]
  sd_f <- tsa_fit$summary["sd F", "estimate"]

  replicate(n_sim, {

    V <- log_F <- out
    
    # transitory noise in year effect, adjusted to allow for more variable years
    
    V[] <- rnorm(n_year, 0, sd_v) * tsa_fit$data$fsd.v.cvmult
    
    # transitory noise in age effect, adjusted to allow more variable mortality in 
    # younger ages
    
    log_F[] <- rnorm(n_year * n_age, 0, sd_f)
    
    log_F <- t(t(log_F) * tsa_fit$data$gudmundssonH1)
    
    exp(log_F + V + F_pers)
  },   
  simplify = FALSE)
}  
  

simulate_N <- function(tsa_fit, F_values, n_min_factor) {

  # simulate numbers at age
  # get recruitment and numbers in starting year from estimates and standard errors
  # then generate other numbers using F and M
  
  y_labels <- get_tsa(tsa_fit, "y_labels")
  a_labels <- get_tsa(tsa_fit, "a_labels")
  
  n <- get_tsa(tsa_fit, "sim_obj")
  
  n_year <- get_tsa(tsa_fit, "n_year")
  n_age <- get_tsa(tsa_fit, "n_age")
  
  
  # simulate numbers at age from estimates and standard errors
  # only want recruitment values and numbers in starting year, but simplest to generate all 
  # and then make the others missing
    
  n[] <- with(tsa_fit, rnorm(length(n$estimate), mean = n$estimate, sd = n$s.error))
    
  # some recruitments are negative (approximations break down) so replace by estimate multiplied by 
  # arbitrary reduction factor n_min
    
  n <- pmax(n, tsa_fit$n$estimate * n_min_factor)

  # make other N's missing
      
  n[y_labels[-1], a_labels[-1]] <- NA 
    

  # estimate N down the cohorts (including plus group)
  
  z <- F_values + tsa_fit$data$auxiliary$natural.mortality
  
  for (iy in 2:n_year) {
    
    n[iy, 2:n_age] <- n[iy-1, 1:(n_age-1)] * exp(- z[iy-1, 1:(n_age-1)])
    
    n[iy, n_age] <- n[iy, n_age] + n[iy-1, n_age] * exp( - z[iy-1, n_age])
  }
    
  list(N_sim = n, F_sim = F_values)
}


simulate_survey <- function(tsa_fit, survey_id, seed, n_sim = 1000) {
  
  #### simulate survey catchabilities and variances from covariance of parameter estimates ####

  require(MASS)
  
  set.seed(seed)

  
  #### covariance matrix ####
  
  # hard to do (numerical difficulties) for full set of parameters, so restrict to 
  # parameters of interest
  
  # need to refit model, so update parameters in setup structure
  
  tsa_setup <- tsa_fit$data
  tsa_setup$param <- tsa.update.param(tsa_setup, tsa_fit, "estimates")$param
  
  # fix parameters not of interest
  
  cv_id <- c("sigma", "eta")
  
  tsa_setup$param <- turn_off_parameters(tsa_setup$param)
  
  tsa_setup$param <- within(tsa_setup$param, {
    survey[[survey_id]]$selection[, "active"] <- 1
    survey[[survey_id]]$cv[cv_id, "active"] <- 1
    survey[[survey_id]]$cv[cv_id, "ndeps"] <- 0.01
  })
  
  # re-run fit (don't worry if it finds a 'better' optimum - easier when just a few parameters)
  # and get hessian, variance matrix, and standard errors
  
  new_fit <- tsa.fit(tsa_setup, dll = "TSA 2017 04 17.dll", hessian = TRUE)

  # extract covariance, give it dimnames and check values are 'sensible'
  
  vcov <- new_fit$vcov

  sel_id <- with(tsa_setup$param, dimnames(survey[[survey_id]]$selection)[[1]])
  
  dimnames(vcov) <- list(c(sel_id, cv_id), c(sel_id, cv_id))

  if (any(is.na(sqrt(diag(vcov))))) 
    warning("non-singular covariance matrix - big think required")

  est_id <- paste(survey_id, c(paste("selection", sel_id), cv_id))
  
  new_summary <- new_fit$summary[est_id, ]
  
  print(new_summary)
  
  
  #### simulate new values ####
  
  out <- mvrnorm(n_sim, new_summary[, "estimate"], vcov)
  
  out <- list(
    selection = out[, sel_id], sigma_eta = out[, cv_id], cv_at_age = NULL,  
    summary = new_summary, vcov = vcov
  )
  
  # ensure sensible values
  
  if (any(out$selection <= 0))
    warning("negative survey selectivties - have a look")
  
  out <- within(out, sigma_eta <- pmax(sigma_eta, 0))
  
  # get approximate cv by age (rather than trying to replicate the 'negative binomial' structure)
  # do this as sigma + eta / sqrt(median(index))
  
  age_id <- with(tsa_setup$survey[[survey_id]], as.character(model$ages))
  
  median_index <- with(tsa_setup$survey[[survey_id]], {
    sapply(age_id, function(i) median(data[, i][as.logical(include[, i])]))
  })
  
  out <- within(out, {
    cv_at_age <- outer(out$sigma_eta[, "sigma"], rep(1, length(age_id))) + 
      outer(out$sigma_eta[, "eta"], 1 / sqrt(median_index))
    dimnames(cv_at_age) <- list(NULL, paste("age", age_id))
  })
  

  out[c("selection", "sigma_eta", "cv_at_age", "summary", "vcov")]
}


turn_off_parameters <- function(param) {
  
  # utility function - fixes all parameters to their original values
  
  sapply(param, simplify = FALSE, FUN = function(x) {
    if (is.matrix(x)) {
      x[, "active"] <- 0
      return(x)
    }
    
    # surveys
    
    sapply(x, simplify = FALSE, FUN = function(ls) {
      ls$selection[, "active"] <- 0
      ls$cv[, "active"] <- 0
      ls
    })
  })
}
  

simulate_catch_cv <- function(tsa_fit, seed, cvmult, n_sim = 1000) {
  
  #### simulate landings and discards cv from covariance of parameter estimates ####
  
  require(MASS)
  
  set.seed(seed)
  
  
  #### covariance matrix ####
  
  # hard to do (numerical difficulties) for full set of parameters, so restrict to 
  # parameters of interest
  
  # need to refit model, so update parameters in setup structure
  
  tsa_setup <- tsa_fit$data
  tsa_setup$param <- tsa.update.param(tsa_setup, tsa_fit, "estimates")$param
  
  # fix parameters not of interest
  
  tsa_setup$param <- turn_off_parameters(tsa_setup$param)
  
  tsa_setup$param <- within(tsa_setup$param, {
    cv[, "active"] <- 1
  })
  
  # re-run fit (don't worry if it finds a 'better' optimum - easier when just a few parameters)
  # and get hessian, variance matrix, and standard errors
  
  new_fit <- tsa.fit(tsa_setup, dll = "TSA 2017 04 17.dll", hessian = TRUE)
  
  # extract covariance, give it dimnames and check values are 'sensible'
  
  vcov <- new_fit$vcov
  
  cv_id <- c("landings", "discards")
  
  dimnames(vcov) <- list(cv_id, cv_id)
  
  if (any(is.na(sqrt(diag(vcov))))) 
    warning("non-singular covariance matrix - big think required")
  
  est_id <- c(paste("cv", cv_id))
  
  new_summary <- new_fit$summary[est_id, ]
  
  print(new_summary)
  
  
  #### simulate new values ####
  
  out <- mvrnorm(n_sim, new_summary[, "estimate"], vcov)
  
  out <- list(cv = out, landings = NULL, discards = NULL, summary = new_summary, vcov = vcov)
  
  # ensure sensible values
  
  out <- within(out, cv <- pmax(cv, 0))
  
  # adjust by cvmult - can't take from model object because we don't want to 
  # build in outliers
  
  age_id <- tsa_setup$model$ages
  
  if (missing(cvmult)) {
    cvmult <- lapply(age_id, function(i) rep(1, times = length(i)))
  }
  
  out[c("landings", "discards")] <- sapply(c("landings", "discards"), function(id) {
    cv_at_age <- outer(out$cv[, id], cvmult[[id]])
    dimnames(cv_at_age) <- list(NULL, paste("age", age_id[[id]]))
    cv_at_age  
  })
  
  
  out
}
